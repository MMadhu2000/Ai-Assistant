


Real-Time AI Assistant (RAG with Ollama & LangChain)

A real-time command-line AI assistant that answers user queries using live web search results combined with a local LLaMA-3 model via Ollama.
This project demonstrates Retrieval-Augmented Generation (RAG) using LangChain.

ğŸ“Œ Overview

This assistant:

Searches the web in real time using DuckDuckGo

Feeds the retrieved content to a local LLM

Generates answers strictly based on search results

Runs entirely on your local machine

If no relevant data is found, the assistant clearly states that.

âœ¨ Features

ğŸ§  Local LLM inference with LLaMA-3 (8B)

ğŸŒ Real-time web search integration

ğŸ”— Retrieval-Augmented Generation (RAG)

ğŸ’¬ Interactive CLI chat loop

âš¡ Simple, lightweight, and beginner-friendly

ğŸ›  Tech Stack

Python 3.9+

LangChain

Ollama

DuckDuckGo Search (ddgs)

ğŸ“‚ Project Structure
.
â”œâ”€â”€ RT_AT.py        # Main application file
â”œâ”€â”€ README.md       # Project documentation
â””â”€â”€ .venv/          # Python virtual environment

ğŸ“¦ Prerequisites
1ï¸âƒ£ Install Ollama

Download and install Ollama from:
ğŸ‘‰ https://ollama.com

2ï¸âƒ£ Pull the LLaMA-3 model
ollama pull llama3:8b

ğŸ”§ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/your-username/real-time-ai-assistant.git
cd real-time-ai-assistant

2ï¸âƒ£ Create and activate a virtual environment
python -m venv .venv
.venv\Scripts\activate   # Windows

3ï¸âƒ£ Install dependencies
pip install langchain langchain-community langchain-ollama ddgs

â–¶ï¸ Usage

Run the assistant using:

python RT_AT.py

![alt text](image.png)

You will see:

ğŸ¤– Hello! I'm a real-time AI assistant. What's new?


Ask questions directly in the terminal.

ğŸ§  How It Works

User enters a question

DuckDuckGo performs a live web search

Search results are passed into a prompt template

LLaMA-3 generates a response only using retrieved data

If results are empty â†’ assistant reports no information found

This ensures grounded and reliable answers.

ğŸ§© Key Concepts

Retrieval-Augmented Generation (RAG)

LangChain Runnable pipelines

Prompt engineering

Local LLM inference with Ollama

ğŸ›‘ Exit the Application

Type:

exit


or

quit

âš ï¸ Notes

Internet is required only for web search

Ollama must be running in the background

Uses LangChain community modules (subject to API updates)

ğŸ“œ License

This project is provided for educational and experimental purposes.

â­ Acknowledgements

LangChain

Ollama

DuckDuckGo